{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression/Classifier of Loan Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages to import\n",
    "from itertools import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "## Install packages if need be\n",
    "#!{sys.executable} -m pip install category_encoders\n",
    "#!{sys.executable} -m pip install scikit-learn\n",
    "#!{sys.executable} -m pip install fancyimpute\n",
    "#!{sys.executable} -m pip install xgboost\n",
    "\n",
    "#Encoders and Imputers\n",
    "from category_encoders import TargetEncoder, OneHotEncoder, HashingEncoder, BinaryEncoder, OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "#Scaling\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "#Plotting and visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid_spec\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn and models\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Data\n",
    "# df = pd.read_csv('.csv')\n",
    "# dg = pd.read_csv('loan_amount.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See datatypes and any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Information about data\n",
    "print(df.info())\n",
    "print(df.dtypes)\n",
    "print(df.country.unique())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split intro train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train and test and validation (Compare some models with validation)\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,dg,test_size=0.20,random_state=42)\n",
    "X_tt,X_val,y_tt,y_val = train_test_split(X_train,y_train,test_size=0.20,random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for correlation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([X_tt,y_tt],axis=1).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCODERS\n",
    "# Too much colision for n_components=8\n",
    "#def hashing_encoding(df_feature):\n",
    "#    '''For state data'''\n",
    "#    df = df_feature.copy()\n",
    "#    he=HashingEncoder(n_components=8)\n",
    "#    return he.fit_transform(df)\n",
    "\n",
    "\n",
    "def binary_encoding(df_feature):\n",
    "    df = df_feature.copy()\n",
    "    be= BinaryEncoder(handle_missing='return_nan',return_df=True)\n",
    "    return be.fit_transform(df)\n",
    "\n",
    "def ordinal_encoding(df_feature):\n",
    "    df = df_feature.copy()\n",
    "    oe = OrdinalEncoder(handle_missing='return_nan',return_df=True)\n",
    "    ddf = oe.fit_transform(df)\n",
    "    ddf.columns = [df_feature.name+'_enc']\n",
    "    return ddf\n",
    "\n",
    "def one_hot_encoding(df_feature):\n",
    "    df = df_feature.copy()\n",
    "    ohe = OneHotEncoder(handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "    return ohe.fit_transform(df)\n",
    "\n",
    "\n",
    "\n",
    "# def encode(X):\n",
    "#     df = X.copy()\n",
    "#     enc = [df,binary_encoding(df['state']),ordinal_encoding(df['is_married']),one_hot_encoding(df['gender']),one_hot_encoding(df['promo_group_1']),one_hot_encoding(df['promo_group_2'])]\n",
    "#     df1 = pd.concat(enc,axis=1)\n",
    "#     X_enc = df1.drop(columns=['state','gender','is_married','promo_group_1','promo_group_2'])\n",
    "#     return X_enc\n",
    "\n",
    "# X_tt_enc = encode(X_tt)\n",
    "# print(X_tt_enc.columns)\n",
    "# X_tt_enc.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute missing data\n",
    "def iter_impute(X):\n",
    "    impute = IterativeImputer(BayesianRidge())\n",
    "    X_imp = pd.DataFrame(impute.fit_transform(X))\n",
    "    X_imp.columns = X.columns\n",
    "    return X_imp\n",
    "\n",
    "\n",
    "X_tt_imp = iter_impute(X_tt_enc)\n",
    "print(X_tt_imp.shape)\n",
    "print(X_tt_imp.columns)\n",
    "print(X_tt_enc.shape)\n",
    "print(X_tt_enc.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(df,col,bins = None,labels = None,*args):\n",
    "    mx = df[col].max()\n",
    "    mn = df[col].min()\n",
    "    if bins == None and labels == None:\n",
    "        step = (mx-mn)/10\n",
    "        b = np.arange(mn,mx,step)\n",
    "        df[col+'_bin']=pd.cut(x = df[col],bins = b,labels = list(range(10)))\n",
    "    elif labels == None:\n",
    "        l = len(bins)\n",
    "        df[col+'_bin']=pd.cut(x = df[col],labels = list(range(l)))\n",
    "    elif bins == None:\n",
    "        step = (mx-mn)/l\n",
    "        b = np.arange(mn,mx,step)\n",
    "        df[col+'_bin']=pd.cut(x = df[col],labels = list(range(l)))\n",
    "    else:\n",
    "        df[col+'_bin']=pd.cut(x = df[col],bins = bins,labels = l)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normalize(df,col):\n",
    "    df[col] = df[col].apply(lambda x: np.log(x+1))\n",
    "    return\n",
    "\n",
    "### Boxplot to get idea of outliers\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=df[col]) #see how skew it is afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim or Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trimming Outliers\n",
    "def trim_outliers(X_tt_imp):\n",
    "    X = X_tt_imp.copy()\n",
    "    X['income'] = X['income'].clip(upper=X_tt_imp.income.quantile(.95),lower=X_tt_imp.income.quantile(.05))\n",
    "    X['age'] = X['age'].clip(upper=X_tt_imp.age.quantile(.95),lower=X_tt_imp.age.quantile(.05))\n",
    "    X['brand_awareness_index'] = X['brand_awareness_index'].clip(upper=X_tt_imp.brand_awareness_index.quantile(.95))\n",
    "    X['customer_loyalty_index'] = X['customer_loyalty_index'].clip(upper=X_tt_imp.customer_loyalty_index.quantile(.95),lower=-X_tt_imp.customer_loyalty_index.quantile(.95))\n",
    "    return X\n",
    "\n",
    "## Dropping outliers (more than 2.5 stddevs from mean)\n",
    "def drop_outliers(X_tt_imp,y_tt):\n",
    "    X = X_tt_imp.copy()\n",
    "    X=X.set_index('customer_id')\n",
    "    y = y_tt.copy()\n",
    "    Xc = pd.concat([X,y],axis=1)\n",
    "    Xc = Xc.drop(Xc[(Xc['income'] > X_tt_imp.income.quantile(.98)) | (Xc['income'] < X_tt_imp.income.quantile(.02)) | (Xc['customer_loyalty_index'] > X_tt_imp.customer_loyalty_index.quantile(.98)) | (Xc['customer_loyalty_index'] < X_tt_imp.customer_loyalty_index.quantile(.02)) | (Xc['brand_awareness_index'] > X_tt_imp.brand_awareness_index.quantile(.95))].index)\n",
    "    Xd = Xc.drop(columns=['tov_6mos'])\n",
    "    yd = Xc.tov_6mos\n",
    "    return Xd,yd\n",
    "\n",
    "## Training data with outliers removed\n",
    "Xd,yd = drop_outliers(X_tt_imp,y_tt)\n",
    "print(Xd.shape)\n",
    "print(yd.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MinMaxScaling not necessary for tree based methods as it is monotonic transformation\n",
    "def scaling(X_tt_imp):    \n",
    "    scaler = MinMaxScaler() \n",
    "    X1 = X_tt_imp.copy()\n",
    "    X1['income'] = scaler.fit_transform(np.array(X1['income']).reshape(-1, 1))\n",
    "    X1['age'] = scaler.fit_transform(np.array(X1['age']).reshape(-1, 1))\n",
    "    X1['customer_loyalty_index'] = scaler.fit_transform(np.array(X1['customer_loyalty_index']).reshape(-1, 1))\n",
    "    X1['brand_awareness_index'] = scaler.fit_transform(np.array(X1['brand_awareness_index']).reshape(-1, 1))\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_impute(X_train1,X_test1):\n",
    "    X_train = X_train1.copy()\n",
    "    X_test = X_test1.copy()\n",
    "    X_test['train'] = 0\n",
    "    X_train['train'] = 1\n",
    "    comb = pd.concat([X_train,X_test])\n",
    "    comb_enc = encode(comb)\n",
    "    comb_imp = iter_impute(comb_enc)\n",
    "    X_test_enc = comb_enc[comb_enc['train'] == 0].drop(columns=['train'])\n",
    "    X_test_imp = comb_imp[comb_imp['train'] == 0].drop(columns=['train'])\n",
    "    return X_test_enc,X_test_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_hyperparam_tune(max_dep,alph,lamb):\n",
    "    xgb_model = XGBRegressor(n_estimators = 200, max_depth=max_dep, min_child_weight=5, gamma=0, eta=0.1, subsample=.75, colsample_bytree=0.8,reg_alpha=alph,reg_lambda=lamb)\n",
    "    xgb_model.fit(Xd,yd,eval_metric='rmse')\n",
    "    y_pred = xgb_model.predict(X_val_imp)\n",
    "    y_tt_pred = xgb_model.predict(Xd)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val,y_pred)\n",
    "    print(\"MAE %f and RMSE %f\" % (mae,rmse))\n",
    "    return y_pred,y_tt_pred\n",
    "\n",
    "max_dep = (5,8)\n",
    "alph = (.05,.1,.2)\n",
    "lamb = (.05,.1,.2)\n",
    "for z in product(max_dep,alph,lamb):\n",
    "    print(z)\n",
    "    y_pred,y_tt_pred= model_hyperparam_tune(*z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using (8, 0.2, 0.2)\n",
    "xgb_model = XGBRegressor(n_estimators = 200, max_depth=8, min_child_weight=5, gamma=0, eta=0.1, subsample=.75, colsample_bytree=0.8,reg_alpha=.2,reg_lambda=.2)\n",
    "xgb_model.fit(X_train_d,y_train_d,eval_metric='rmse')\n",
    "y_pred = xgb_model.predict(X_test_imp)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "print(\"MAE %f and RMSE %f\" % (mae,rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Importance\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.rcParams['figure.figsize'] = [9, 9]\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
